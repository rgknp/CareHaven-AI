{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for LightGBM Baseline\n",
    "\n",
    "This notebook demonstrates how to engineer temporal, volatility, and derivative features for time series modeling using LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-prometheus 0.56b0 requires opentelemetry-sdk~=1.35.0, but you have opentelemetry-sdk 1.36.0 which is incompatible.\n",
      "mlflow-skinny 2.21.3 requires packaging<25, but you have packaging 25.0 which is incompatible.\n",
      "jupyterlab-nvdashboard 0.13.0 requires jupyterlab>=4, but you have jupyterlab 3.6.8 which is incompatible.\n",
      "jupyter-resource-usage 0.7.2 requires psutil~=5.6, but you have psutil 7.0.0 which is incompatible.\n",
      "dask-sql 2024.5.0 requires dask[dataframe]>=2024.4.1, but you have dask 2023.2.0 which is incompatible.\n",
      "dask-sql 2024.5.0 requires distributed>=2024.4.1, but you have distributed 2023.2.0 which is incompatible.\n",
      "azureml-training-tabular 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.0.0 which is incompatible.\n",
      "azureml-training-tabular 1.60.0 requires scipy<1.11.0,>=1.0.0, but you have scipy 1.11.0 which is incompatible.\n",
      "azureml-training-tabular 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\n",
      "azureml-mlflow 1.60.0 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.26.0 which is incompatible.\n",
      "azureml-automl-runtime 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.0.0 which is incompatible.\n",
      "azureml-automl-runtime 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\n",
      "azureml-automl-dnn-nlp 1.60.0 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\n",
      "adlfs 2024.12.0 requires fsspec>=2023.12.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Ensure required Azure ML data access packages are installed (run once; restart kernel if versions upgrade)\n",
    "%pip install -q azure-ai-ml azure-identity azureml-fsspec mltable\n",
    "\n",
    "# After installation, you may need to restart the kernel if this is the first time installing these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: __file__ undefined in notebook context; using CWD: /mnt/batch/tasks/shared/LS_root/mounts/clusters/rgknp1/code\n",
      "Loaded CONFIG keys: ['LAGS', 'ROLLING_WINDOWS', 'SLOPE_WINDOW', 'SPIKE_Z_THRESHOLD', 'MIN_HISTORY_DAYS', 'SEED', 'N_FOLDS', 'LGB_PARAMS']\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters (loaded from JSON if present)\n",
    "import json, random, os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    # In notebooks, __file__ may be undefined; fall back to current working directory\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path(os.getcwd())\n",
    "    print('Note: __file__ undefined in notebook context; using CWD:', BASE_DIR)\n",
    "params_path = BASE_DIR / 'featureengineering_params.json'\n",
    "if params_path.exists():\n",
    "    with open(params_path,'r') as f:\n",
    "        CONFIG = json.load(f)\n",
    "else:\n",
    "    CONFIG = {\n",
    "        'LAGS':[1,7,14],\n",
    "        'ROLLING_WINDOWS':[7,14,30],\n",
    "        'SLOPE_WINDOW':30,\n",
    "        'SPIKE_Z_THRESHOLD':2.5,\n",
    "        'MIN_HISTORY_DAYS':14,\n",
    "        'SEED':42,\n",
    "        'N_FOLDS':5,\n",
    "        'LGB_PARAMS': {'objective':'regression','metric':['mae','rmse'],'learning_rate':0.03}\n",
    "    }\n",
    "    # Optionally write default file for reproducibility\n",
    "    try:\n",
    "        with open(params_path,'w') as f:\n",
    "            json.dump(CONFIG,f,indent=2)\n",
    "        print('Wrote default params to', params_path)\n",
    "    except Exception as e:\n",
    "        print('Could not write default params file:', e)\n",
    "print('Loaded CONFIG keys:', list(CONFIG.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM not yet imported or available: Logger must provide 'info' and 'warning' method\n",
      "Seeds set to 42\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility seeds\n",
    "import numpy as np, random, os\n",
    "SEED = CONFIG.get('SEED', 42)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    lgb.register_logger(lambda msg: None)\n",
    "except Exception as e:\n",
    "    print('LightGBM not yet imported or available:', e)\n",
    "print('Seeds set to', SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n",
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tManagedIdentityCredential: string indices must be integers\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n",
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tManagedIdentityCredential: string indices must be integers\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to retrieve data asset multidomain_cognitive_dataset v1: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tManagedIdentityCredential: string indices must be integers\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\nConfirm it exists under Assets > Data in the workspace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientAuthenticationError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     data_asset \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_ASSET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_ASSET_VERSION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved data asset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ASSET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ASSET_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    286\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    287\u001b[0m         ):\n\u001b[0;32m--> 288\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/ai/ml/operations/_data_operations.py:276\u001b[0m, in \u001b[0;36mDataOperations.get\u001b[0;34m(self, name, version, label)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[1;32m    270\u001b[0m         message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    271\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mDATA,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mMISSING_FIELD,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 276\u001b[0m data_version_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Data\u001b[38;5;241m.\u001b[39m_from_rest_object(data_version_resource)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/ai/ml/operations/_data_operations.py:205\u001b[0m, in \u001b[0;36mDataOperations._get\u001b[0;34m(self, name, version)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    198\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    199\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    200\u001b[0m             registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    201\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_kwargs,\n\u001b[1;32m    203\u001b[0m         )\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_operation\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    215\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/tracing/decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_data_versions_operations.py:479\u001b[0m, in \u001b[0;36mDataVersionsOperations.get\u001b[0;34m(self, resource_group_name, workspace_name, name, version, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m request\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mformat_url(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m--> 479\u001b[0m pipeline_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/_base.py:242\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/mgmt/core/policies/_base.py:95\u001b[0m, in \u001b[0;36mARMAutoResourceProviderRegistrationPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m http_request \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mhttp_request\n\u001b[0;32m---> 95\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mhttp_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_redirect.py:205\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 205\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_retry.py:544\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    543\u001b[0m request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 544\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_authentication.py:157\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m:param request: The pipeline request object\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_authentication.py:132\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.on_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_new_token:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m bearer_token \u001b[38;5;241m=\u001b[39m cast(Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessToken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessTokenInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token)\u001b[38;5;241m.\u001b[39mtoken\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_authentication.py:108\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._request_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Request a new token from the credential.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mThis will call the credential's appropriate method to get a token and store it in the policy.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m:param str scopes: The type of access needed.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/core/pipeline/policies/_authentication.py:98\u001b[0m, in \u001b[0;36m_BearerTokenCredentialPolicyBase._get_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             options[key] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(key)  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSupportsTokenInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_credential\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(TokenCredential, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential)\u001b[38;5;241m.\u001b[39mget_token(\u001b[38;5;241m*\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/identity/_credentials/default.py:256\u001b[0m, in \u001b[0;36mDefaultAzureCredential.get_token_info\u001b[0;34m(self, options, *scopes)\u001b[0m\n\u001b[1;32m    255\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 256\u001b[0m token_info \u001b[38;5;241m=\u001b[39m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSupportsTokenInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m within_dac\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/azure/identity/_credentials/chained.py:219\u001b[0m, in \u001b[0;36mChainedTokenCredential.get_token_info\u001b[0;34m(self, options, *scopes)\u001b[0m\n\u001b[1;32m    218\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientAuthenticationError(message\u001b[38;5;241m=\u001b[39mmessage)\n",
      "\u001b[0;31mClientAuthenticationError\u001b[0m: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tManagedIdentityCredential: string indices must be integers\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsset path URI:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_asset\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve data asset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ASSET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ASSET_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfirm it exists under Assets > Data in the workspace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# The asset is a single JSON file (file asset). We can open it directly using fsspec-style open.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# azureml-fsspec registers an implementation so pandas / open() can consume it.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to retrieve data asset multidomain_cognitive_dataset v1: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tManagedIdentityCredential: string indices must be integers\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\nConfirm it exists under Assets > Data in the workspace."
     ]
    }
   ],
   "source": [
    "# Load and flatten multidomain cognitive JSON dataset via Azure ML Data Asset (SDK v2) with credential fallbacks\n",
    "# Official guidance: https://learn.microsoft.com/azure/machine-learning/tutorial-explore-data?view=azureml-api-2\n",
    "# This cell now attempts multiple auth strategies: DefaultAzureCredential -> AzureCliCredential -> InteractiveBrowserCredential.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    AzureCliCredential,\n",
    "    InteractiveBrowserCredential\n",
    ")\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.core.exceptions import ClientAuthenticationError\n",
    "\n",
    "# --- CONFIGURE: name & version of the registered data asset ---\n",
    "DATA_ASSET_NAME = \"multidomain_cognitive_dataset\"  # adjust to actual asset name\n",
    "DATA_ASSET_VERSION = \"1\"  # adjust version\n",
    "\n",
    "SUBSCRIPTION_ID = \"5944e6be-7b60-49c8-886b-307896de21f9\"\n",
    "RESOURCE_GROUP = \"rahgupt-hackathon25\"\n",
    "WORKSPACE_NAME = \"carehaven-ai-workspace\"\n",
    "\n",
    "credential_attempts = []\n",
    "ml_client = None\n",
    "\n",
    "# Try DefaultAzureCredential first\n",
    "try:\n",
    "    cred = DefaultAzureCredential(exclude_interactive_browser_credential=True)\n",
    "    ml_client = MLClient(credential=cred, subscription_id=SUBSCRIPTION_ID, resource_group_name=RESOURCE_GROUP, workspace_name=WORKSPACE_NAME)\n",
    "    # simple call to ensure it works\n",
    "    _ = ml_client.workspaces.get(WORKSPACE_NAME)\n",
    "    credential_attempts.append(\"DefaultAzureCredential: SUCCESS\")\n",
    "except Exception as e:\n",
    "    credential_attempts.append(f\"DefaultAzureCredential: {type(e).__name__}: {e}\")\n",
    "    ml_client = None\n",
    "\n",
    "# Fallback: Azure CLI cached token (requires az login on compute)\n",
    "if ml_client is None:\n",
    "    try:\n",
    "        cred = AzureCliCredential()\n",
    "        ml_client = MLClient(credential=cred, subscription_id=SUBSCRIPTION_ID, resource_group_name=RESOURCE_GROUP, workspace_name=WORKSPACE_NAME)\n",
    "        _ = ml_client.workspaces.get(WORKSPACE_NAME)\n",
    "        credential_attempts.append(\"AzureCliCredential: SUCCESS\")\n",
    "    except Exception as e:\n",
    "        credential_attempts.append(f\"AzureCliCredential: {type(e).__name__}: {e}\")\n",
    "        ml_client = None\n",
    "\n",
    "# Fallback: Interactive browser (will open auth flow; may not work in headless runs)\n",
    "if ml_client is None:\n",
    "    try:\n",
    "        cred = InteractiveBrowserCredential()\n",
    "        ml_client = MLClient(credential=cred, subscription_id=SUBSCRIPTION_ID, resource_group_name=RESOURCE_GROUP, workspace_name=WORKSPACE_NAME)\n",
    "        _ = ml_client.workspaces.get(WORKSPACE_NAME)\n",
    "        credential_attempts.append(\"InteractiveBrowserCredential: SUCCESS\")\n",
    "    except Exception as e:\n",
    "        credential_attempts.append(f\"InteractiveBrowserCredential: {type(e).__name__}: {e}\")\n",
    "        ml_client = None\n",
    "\n",
    "if ml_client is None:\n",
    "    diag = \"\\n\".join(credential_attempts)\n",
    "    raise ClientAuthenticationError(\n",
    "        message=(\n",
    "            \"All credential attempts failed.\\n\" + diag +\n",
    "            \"\\nTroubleshooting steps:\\n\"\n",
    "            \"  1. If using compute instance, ensure you are signed in: open a terminal and run 'az login'.\\n\"\n",
    "            \"  2. If using managed identity, attach a User Assigned Managed Identity and grant 'AzureML Data Scientist' or Reader roles.\\n\"\n",
    "            \"  3. For service principal, set environment variables AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET and re-run.\\n\"\n",
    "            \"  4. As a last resort, enable InteractiveBrowserCredential (already attempted) ensuring UI login possible.\\n\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"Authentication succeeded via:\", [a for a in credential_attempts if a.endswith('SUCCESS')][0])\n",
    "\n",
    "# Retrieve the data asset metadata\n",
    "try:\n",
    "    data_asset = ml_client.data.get(name=DATA_ASSET_NAME, version=DATA_ASSET_VERSION)\n",
    "    print(f\"Retrieved data asset '{DATA_ASSET_NAME}' v{DATA_ASSET_VERSION}\")\n",
    "    print('Asset path URI:', data_asset.path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to retrieve data asset {DATA_ASSET_NAME} v{DATA_ASSET_VERSION}: {e}\\n\" \\\n",
    "        \"Confirm the asset exists under Assets > Data or update name/version.\"\n",
    "    )\n",
    "\n",
    "# Determine if asset is a file or folder; handle JSON file(s).\n",
    "import fsspec\n",
    "fs, _, paths = fsspec.get_fs_token_paths(data_asset.path)\n",
    "if len(paths) == 0:\n",
    "    raise FileNotFoundError(\"Data asset path resolved to zero files. Verify the asset contents.\")\n",
    "if len(paths) > 1:\n",
    "    print('Note: multiple files detected; concatenating all JSON arrays.')\n",
    "\n",
    "records_raw = []\n",
    "for p in paths:\n",
    "    with fs.open(p, 'r') as f:\n",
    "        try:\n",
    "            payload = json.load(f)\n",
    "        except json.JSONDecodeError as jde:\n",
    "            raise ValueError(f\"File {p} is not valid JSON: {jde}\")\n",
    "        if isinstance(payload, list):\n",
    "            records_raw.extend(payload)\n",
    "        else:\n",
    "            raise ValueError(f\"JSON root in {p} not a list; adapt loader for object-based format.\")\n",
    "\n",
    "print('Total session records loaded:', len(records_raw))\n",
    "if not records_raw:\n",
    "    raise ValueError('No session records found after reading asset files.')\n",
    "\n",
    "# Flatten nested structure\n",
    "domains = ['attention','executive_function','memory','orientation','processing_speed','mood_behavior']\n",
    "flat_rows = []\n",
    "for rec in records_raw:\n",
    "    base = {\n",
    "        'device_id': rec.get('device_id'),\n",
    "        'patient_id': rec.get('patient_id'),\n",
    "        'session_date': rec.get('session_date')\n",
    "    }\n",
    "    for d in domains:\n",
    "        dct = rec.get(d, {}) or {}\n",
    "        for k,v in dct.items():\n",
    "            base[f\"{d}__{k}\"] = v\n",
    "    flat_rows.append(base)\n",
    "\n",
    "df = pd.DataFrame(flat_rows)\n",
    "if df.empty:\n",
    "    raise ValueError('Resulting DataFrame is empty after flattening.')\n",
    "\n",
    "# Date handling and ordering\n",
    "df['session_date'] = pd.to_datetime(df['session_date'])\n",
    "df = df.sort_values(['patient_id','session_date']).reset_index(drop=True)\n",
    "print('DataFrame shape:', df.shape)\n",
    "print('Preview:')\n",
    "display(df.head(3))\n",
    "print('Columns:', len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features based on session_date\n",
    "df['days_since_baseline'] = df.groupby('patient_id')['session_date'].transform(lambda s: (s - s.min()).dt.days)\n",
    "df['dayofweek'] = df['session_date'].dt.dayofweek\n",
    "df['weekofyear'] = df['session_date'].dt.isocalendar().week.astype(int)\n",
    "df['month'] = df['session_date'].dt.month\n",
    "df['is_weekend'] = (df['dayofweek'] >=5).astype(int)\n",
    "df['sin_dayofweek'] = np.sin(2*np.pi*df['dayofweek']/7)\n",
    "df['cos_dayofweek'] = np.cos(2*np.pi*df['dayofweek']/7)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct composite cognitive_health_index if absent\n",
    "target_col = 'cognitive_health_index'\n",
    "if target_col not in df.columns:\n",
    "    # Select representative domain metrics (normalize each to 0-1 via min-max per cohort)\n",
    "    component_cols = [c for c in df.columns if any(prefix in c for prefix in ['attention__','executive_function__','memory__','processing_speed__','mood_behavior__'])]\n",
    "    comp_df = df[component_cols].copy()\n",
    "    # Coerce booleans to int\n",
    "    for c in comp_df.columns:\n",
    "        if comp_df[c].dtype == bool:\n",
    "            comp_df[c] = comp_df[c].astype(int)\n",
    "    # Min-max scale across all patients (could be per-patient or robust scaled later)\n",
    "    comp_min = comp_df.min()\n",
    "    comp_range = (comp_df.max() - comp_min).replace(0,1)\n",
    "    comp_scaled = (comp_df - comp_min)/comp_range\n",
    "    # Weighted average: equal weights for now\n",
    "    df[target_col] = comp_scaled.mean(axis=1)\n",
    "    # Slight shrink away from 0/1 for stability\n",
    "    eps = 1e-4\n",
    "    df[target_col] = df[target_col].clip(eps,1-eps)\n",
    "print(df[[target_col]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility & Advanced Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering utility functions\n",
    "from typing import Sequence\n",
    "def add_lags(data, group_col: str, date_col: str, features: Sequence[str], lags=(1, 7, 14)):\n",
    "    data = data.sort_values([group_col, date_col])\n",
    "    for f in features:\n",
    "        for L in lags:\n",
    "            data[f\"{f}_lag{L}\"] = data.groupby(group_col)[f].shift(L)\n",
    "    return data\n",
    "\n",
    "def add_rolling_stats(data, group_col: str, date_col: str, features: Sequence[str], windows=(7, 14, 30)):\n",
    "    data = data.sort_values([group_col, date_col])\n",
    "    for f in features:\n",
    "        g = data.groupby(group_col)[f]\n",
    "        for w in windows:\n",
    "            data[f\"roll_mean_{f}_{w}\"] = g.transform(lambda s: s.rolling(w, min_periods=max(3, int(w/2))).mean())\n",
    "            data[f\"roll_std_{f}_{w}\"]  = g.transform(lambda s: s.rolling(w, min_periods=max(3, int(w/2))).std())\n",
    "    return data\n",
    "\n",
    "def add_ratios(df):\n",
    "    if 'memory__delayed_recall' in df and 'memory__immediate_recall' in df:\n",
    "        df['memory_retention_ratio'] = df['memory__delayed_recall'] / (df['memory__immediate_recall'] + 1e-6)\n",
    "    if 'attention__digit_span_max' in df and 'attention__latency_sec' in df:\n",
    "        df['attention_efficiency'] = df['attention__digit_span_max'] / (df['attention__latency_sec'] + 1e-6)\n",
    "    if 'executive_function__verbal_fluency_words' in df and 'executive_function__avg_pause_ms' in df:\n",
    "        df['executive_fluency_efficiency'] = df['executive_function__verbal_fluency_words'] / (df['executive_function__avg_pause_ms'] + 1e-3)\n",
    "    return df\n",
    "\n",
    "def add_slopes(data, group_col: str, date_col: str, feature: str, window=30):\n",
    "    import numpy as np\n",
    "    data = data.sort_values([group_col, date_col])\n",
    "    def rolling_slope(series):\n",
    "        idx = np.arange(len(series))\n",
    "        out = np.full(len(series), np.nan)\n",
    "        for i in range(window - 1, len(series)):\n",
    "            y = series.iloc[i - window + 1:i + 1].values\n",
    "            x = idx[i - window + 1:i + 1]\n",
    "            x = x - x.mean()\n",
    "            denom = (x ** 2).sum()\n",
    "            if denom == 0:\n",
    "                continue\n",
    "            out[i] = (x * y).sum() / denom\n",
    "        return out\n",
    "    data[f\"local_slope_{feature}_{window}\"] = data.groupby(group_col)[feature].transform(rolling_slope)\n",
    "    return data\n",
    "\n",
    "def add_volatility_flags(df, base_feature: str, window=7, z_threshold=2.5):\n",
    "    mean_col = f\"roll_mean_{base_feature}_{window}\"\n",
    "    std_col = f\"roll_std_{base_feature}_{window}\"\n",
    "    if mean_col in df and std_col in df:\n",
    "        df[f\"spike_{base_feature}\"] = (df[base_feature] - df[mean_col]) / (df[std_col] + 1e-6)\n",
    "        df[f\"spike_flag_{base_feature}\"] = (df[f\"spike_{base_feature}\"] > z_threshold).astype(int)\n",
    "    return df\n",
    "\n",
    "print('Utility functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering pipeline\n",
    "target = 'cognitive_health_index'\n",
    "numeric_features = [c for c in df.columns if any(prefix in c for prefix in ['attention__','executive_function__','memory__','processing_speed__','mood_behavior__']) and df[c].dtype!=object]\n",
    "df = add_ratios(df)\n",
    "# Recompute numeric list including ratios\n",
    "extended_numeric = numeric_features + [c for c in ['memory_retention_ratio','attention_efficiency','executive_fluency_efficiency'] if c in df.columns]\n",
    "df = add_lags(df,'patient_id','session_date', extended_numeric, lags=(1,7,14))\n",
    "df = add_rolling_stats(df,'patient_id','session_date', extended_numeric, windows=(7,14,30))\n",
    "# Volatility flags for a few sentinel features\n",
    "sentinel_feats = [f for f in ['processing_speed__avg_reaction_time_ms','memory__immediate_recall','executive_function__verbal_fluency_words'] if f in df.columns]\n",
    "for sf in sentinel_feats:\n",
    "    df = add_volatility_flags(df, sf, window=7, z_threshold=2.5)\n",
    "df = add_slopes(df,'patient_id','session_date', target, window=30)\n",
    "# Cumulative decline from patient baseline\n",
    "baseline_target = df.groupby('patient_id')[target].transform('first')\n",
    "df['cumulative_decline'] = baseline_target - df[target]\n",
    "# Drop early rows with insufficient history (optional)\n",
    "min_history_days = 14\n",
    "df_filtered = df[df['days_since_baseline'] >= min_history_days].copy()\n",
    "print('Original rows:', len(df), 'Filtered rows:', len(df_filtered))\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for LightGBM\n",
    "id_cols = ['patient_id','device_id','session_date']\n",
    "drop_cols = id_cols + ['cognitive_health_index']\n",
    "feature_cols = [c for c in df_filtered.columns if c not in drop_cols]\n",
    "X = df_filtered[feature_cols]\n",
    "y = df_filtered['cognitive_health_index']\n",
    "print('Feature count:', len(feature_cols))\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "print('Sample features:', feature_cols[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic quality checks\n",
    "missing_summary = df_filtered[feature_cols].isna().mean().sort_values(ascending=False).head(10)\n",
    "print('Top 10 missing feature proportions:\\n', missing_summary)\n",
    "print('Describe target:')\n",
    "print(y.describe())\n",
    "print('First 3 rows of engineered features:')\n",
    "display(df_filtered[id_cols + ['cognitive_health_index'] + feature_cols[:5]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist final stacked model (simple average for now)\n",
    "import pickle\n",
    "model_path = Path(__file__).resolve().parent / 'lgbm_groupcv_models.pkl'\n",
    "with open(model_path,'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "print('Saved individual fold models to', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Grouped CV Training\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math, json\n",
    "groups = df_filtered['patient_id'].values\n",
    "N_FOLDS = CONFIG.get('N_FOLDS',5)\n",
    "params = CONFIG['LGB_PARAMS']\n",
    "fold_mae = []\n",
    "fold_rmse = []\n",
    "models = []\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "for fold,(tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups),1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_valid = lgb.Dataset(X_va, label=y_va, reference=lgb_train)\n",
    "    booster = lgb.train(params, lgb_train, num_boost_round=4000, valid_sets=[lgb_train, lgb_valid],\n",
    "                        valid_names=['train','valid'], early_stopping_rounds=150, verbose_eval=False)\n",
    "    pred = booster.predict(X_va, num_iteration=booster.best_iteration)\n",
    "    mae = mean_absolute_error(y_va, pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_va, pred))\n",
    "    fold_mae.append(mae); fold_rmse.append(rmse)\n",
    "    models.append(booster)\n",
    "    print(f'Fold {fold}: MAE={mae:.4f} RMSE={rmse:.4f} BestIter={booster.best_iteration}')\n",
    "print('Overall MAE mean±std:', f'{np.mean(fold_mae):.4f} ± {np.std(fold_mae):.4f}')\n",
    "print('Overall RMSE mean±std:', f'{np.mean(fold_rmse):.4f} ± {np.std(fold_rmse):.4f}')\n",
    "# Feature importance (gain) aggregate\n",
    "importances = {}\n",
    "for m in models:\n",
    "    for feat, imp in zip(m.feature_name(), m.feature_importance(importance_type='gain')):\n",
    "        importances[feat] = importances.get(feat,0) + imp\n",
    "sorted_feats = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:40]\n",
    "print('Top 20 features:')\n",
    "for f,i in sorted_feats[:20]:\n",
    "    print(f'{f}: {i:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export engineered features\n",
    "output_path = Path(__file__).resolve().parent / 'engineered_cognitive_features.csv'\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "print('Saved engineered features to', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist feature list artifact\n",
    "feature_list_path = Path(__file__).resolve().parent / 'feature_list.json'\n",
    "with open(feature_list_path,'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print('Saved feature list to', feature_list_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
